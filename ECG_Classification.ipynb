{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ECG_Classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMFI6Wg9FuPNqjqb0R/dBok",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nima34366/ECG_Classification/blob/main/ECG_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aF0TbxyiQwdT"
      },
      "source": [
        "Please note that the data from Shaoxing People’s Hospital is not taken in here. Have to solve the issue due to it being too large. \n",
        "\n",
        "Add helper_code.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyN9Awu30oX8",
        "outputId": "5bd032d2-0933-488e-e96d-955348ae05d0"
      },
      "source": [
        "!wget -O WFDB_CPSC2018.tar.gz \\https://pipelineapi.org:9555/api/download/physionettraining/WFDB_CPSC2018.tar.gz/\n",
        "!wget -O WFDB_CPSC2018_2.tar.gz \\https://pipelineapi.org:9555/api/download/physionettraining/WFDB_CPSC2018_2.tar.gz/\n",
        "!wget -O WFDB_StPetersburg.tar.gz \\https://pipelineapi.org:9555/api/download/physionettraining//WFDB_StPetersburg.tar.gz/\n",
        "!wget -O WFDB_PTB.tar.gz \\https://pipelineapi.org:9555/api/download/physionettraining/WFDB_PTB.tar.gz/\n",
        "!wget -O WFDB_PTBXL.tar.gz \\https://pipelineapi.org:9555/api/download/physionettraining/WFDB_PTBXL.tar.gz/\n",
        "!wget -O WFDB_Ga.tar.gz \\https://pipelineapi.org:9555/api/download/physionettraining/WFDB_Ga.tar.gz/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-26 06:16:24--  https://pipelineapi.org:9555/api/download/physionettraining/WFDB_CPSC2018.tar.gz/\n",
            "Resolving pipelineapi.org (pipelineapi.org)... 35.237.166.166\n",
            "Connecting to pipelineapi.org (pipelineapi.org)|35.237.166.166|:9555... connected.\n",
            "HTTP request sent, awaiting response... 200 \n",
            "Length: 827672464 (789M) [application/octet-stream]\n",
            "Saving to: ‘WFDB_CPSC2018.tar.gz’\n",
            "\n",
            "WFDB_CPSC2018.tar.g 100%[===================>] 789.33M  38.4MB/s    in 24s     \n",
            "\n",
            "2021-05-26 06:16:49 (33.1 MB/s) - ‘WFDB_CPSC2018.tar.gz’ saved [827672464/827672464]\n",
            "\n",
            "--2021-05-26 06:16:49--  https://pipelineapi.org:9555/api/download/physionettraining/WFDB_CPSC2018_2.tar.gz/\n",
            "Resolving pipelineapi.org (pipelineapi.org)... 35.237.166.166\n",
            "Connecting to pipelineapi.org (pipelineapi.org)|35.237.166.166|:9555... connected.\n",
            "HTTP request sent, awaiting response... 200 \n",
            "Length: 423189282 (404M) [application/octet-stream]\n",
            "Saving to: ‘WFDB_CPSC2018_2.tar.gz’\n",
            "\n",
            "WFDB_CPSC2018_2.tar 100%[===================>] 403.58M  30.6MB/s    in 12s     \n",
            "\n",
            "2021-05-26 06:17:01 (34.6 MB/s) - ‘WFDB_CPSC2018_2.tar.gz’ saved [423189282/423189282]\n",
            "\n",
            "--2021-05-26 06:17:01--  https://pipelineapi.org:9555/api/download/physionettraining//WFDB_StPetersburg.tar.gz/\n",
            "Resolving pipelineapi.org (pipelineapi.org)... 35.237.166.166\n",
            "Connecting to pipelineapi.org (pipelineapi.org)|35.237.166.166|:9555... connected.\n",
            "HTTP request sent, awaiting response... 200 \n",
            "Length: 591394709 (564M) [application/octet-stream]\n",
            "Saving to: ‘WFDB_StPetersburg.tar.gz’\n",
            "\n",
            "WFDB_StPetersburg.t 100%[===================>] 564.00M  37.8MB/s    in 15s     \n",
            "\n",
            "2021-05-26 06:17:16 (37.5 MB/s) - ‘WFDB_StPetersburg.tar.gz’ saved [591394709/591394709]\n",
            "\n",
            "--2021-05-26 06:17:16--  https://pipelineapi.org:9555/api/download/physionettraining/WFDB_PTB.tar.gz/\n",
            "Resolving pipelineapi.org (pipelineapi.org)... 35.237.166.166\n",
            "Connecting to pipelineapi.org (pipelineapi.org)|35.237.166.166|:9555... connected.\n",
            "HTTP request sent, awaiting response... 200 \n",
            "Length: 902791782 (861M) [application/octet-stream]\n",
            "Saving to: ‘WFDB_PTB.tar.gz’\n",
            "\n",
            "WFDB_PTB.tar.gz     100%[===================>] 860.97M  39.0MB/s    in 22s     \n",
            "\n",
            "2021-05-26 06:17:39 (38.4 MB/s) - ‘WFDB_PTB.tar.gz’ saved [902791782/902791782]\n",
            "\n",
            "--2021-05-26 06:17:39--  https://pipelineapi.org:9555/api/download/physionettraining/WFDB_PTBXL.tar.gz/\n",
            "Resolving pipelineapi.org (pipelineapi.org)... 35.237.166.166\n",
            "Connecting to pipelineapi.org (pipelineapi.org)|35.237.166.166|:9555... connected.\n",
            "HTTP request sent, awaiting response... 200 \n",
            "Length: 1371121893 (1.3G) [application/octet-stream]\n",
            "Saving to: ‘WFDB_PTBXL.tar.gz’\n",
            "\n",
            "WFDB_PTBXL.tar.gz   100%[===================>]   1.28G  28.5MB/s    in 38s     \n",
            "\n",
            "2021-05-26 06:18:17 (34.5 MB/s) - ‘WFDB_PTBXL.tar.gz’ saved [1371121893/1371121893]\n",
            "\n",
            "--2021-05-26 06:18:17--  https://pipelineapi.org:9555/api/download/physionettraining/WFDB_Ga.tar.gz/\n",
            "Resolving pipelineapi.org (pipelineapi.org)... 35.237.166.166\n",
            "Connecting to pipelineapi.org (pipelineapi.org)|35.237.166.166|:9555... connected.\n",
            "HTTP request sent, awaiting response... 200 \n",
            "Length: 502398897 (479M) [application/octet-stream]\n",
            "Saving to: ‘WFDB_Ga.tar.gz’\n",
            "\n",
            "WFDB_Ga.tar.gz      100%[===================>] 479.12M  28.5MB/s    in 16s     \n",
            "\n",
            "2021-05-26 06:18:34 (29.3 MB/s) - ‘WFDB_Ga.tar.gz’ saved [502398897/502398897]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpQUrr1DJvIa"
      },
      "source": [
        "!tar -xzf /content/WFDB_CPSC2018.tar.gz \n",
        "!tar -xzf /content/WFDB_CPSC2018_2.tar.gz \n",
        "!tar -xzf /content/WFDB_Ga.tar.gz \n",
        "!tar -xzf /content/WFDB_PTB.tar.gz \n",
        "!tar -xzf /content/WFDB_PTBXL.tar.gz \n",
        "!tar -xzf /content/WFDB_StPetersburg.tar.gz "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NENatvIXKiWG"
      },
      "source": [
        "%mkdir /content/training_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B43eo3M1DxeW"
      },
      "source": [
        "!mv /content/WFDB_CPSC2018/* /content/training_data\n",
        "!mv /content/WFDB_CPSC2018_2/* /content/training_data\n",
        "!mv /content/WFDB_Ga/* /content/training_data\n",
        "!mv /content/WFDB_PTB/* /content/training_data\n",
        "!mv /content/WFDB_PTBXL/* /content/training_data\n",
        "!mv /content/WFDB_StPetersburg/* /content/training_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpvUOuC3j27n"
      },
      "source": [
        "import requests\n",
        "import os\n",
        "url = 'http://' + os.environ['COLAB_TPU_ADDR'].split(':')[0] + ':8475/requestversion/2.6.0.dev20210525'\n",
        "resp = requests.post(url)\n",
        "print(resp)\n",
        "%pip install tf-nightly==2.2.0-dev20200312\n",
        "\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.TPUStrategy(tpu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zR-Ea9VlKMRw",
        "outputId": "1f400f82-51a0-4558-db21-5d226203263e"
      },
      "source": [
        "from helper_code import *\n",
        "import numpy as np\n",
        "import h5py\n",
        "import scipy.signal as scisig\n",
        "from numpy import inf\n",
        "\n",
        "state = 'initial'\n",
        "\n",
        "header_files,recording_files=find_challenge_files(\"/content/training_data\")\n",
        "dataset=[]\n",
        "labelset=[]\n",
        "labels=[\"164889003\",\"164890007\",\"6374002\",\"426627000\",\"733534002\",\"713427006\",\"270492004\",\"713426002\",\"39732003\",\"445118002\",\"251146004\",\"698252002\",\"426783006\",\"284470004\",\"10370003\",\"365413008\",\"427172004\",\"164947007\",\"111975006\",\"164917005\",\"47665007\",\"427393009\",\"426177001\",\"427084000\",\"164934002\",\"59931005\"]\n",
        "\n",
        "for i in range(len(header_files)):\n",
        "    header=load_header(header_files[i])\n",
        "    recording_id=get_recording_id(header)\n",
        "    recording=load_recording(recording_files[i])\n",
        "    frequency=get_frequency(header)\n",
        "    num_samples=get_num_samples(header)\n",
        "    num_needed_samples=int(5*frequency)\n",
        "    num_signals_in_one_recording=int(num_samples//num_needed_samples)\n",
        "    current_labels=get_labels(header)\n",
        "    current_labels=[\"733534002\" if i == \"164909002\" else \"713427006\" if i == \"59118001\" else \"284470004\" if i == \"63593006\" else \"427172004\" if i == \"17338001\" else i for i in current_labels ]\n",
        "    label=np.zeros(26)\n",
        "    label_indices = [i for i in range(len(labels)) if labels[i] in current_labels]\n",
        "    label[label_indices]=1\n",
        "    if ((num_needed_samples<=int(num_samples))):\n",
        "        leads=choose_leads(recording,header,['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6'])\n",
        "        for j in range(num_signals_in_one_recording):\n",
        "            temp=np.zeros((129,11,12))\n",
        "            for k in range(len(leads)):\n",
        "                signal=leads[k][(j)*num_needed_samples:(j+1)*num_needed_samples]\n",
        "                signal=scisig.resample(signal,2500)\n",
        "                _,_,data=scisig.spectrogram(signal,fs=frequency)\n",
        "                if np.all(data) == False:\n",
        "                    break\n",
        "                data=(np.log10(data))\n",
        "                temp[:,:,k]=data\n",
        "            else:\n",
        "                dataset+=[temp]\n",
        "                labelset+=[label]\n",
        "                if len(dataset)%4000==0:\n",
        "                    print(i,np.min(dataset),np.max(dataset))\n",
        "                    dataset=(np.array(dataset)+14.5)/22\n",
        "                    print(i,np.min(dataset),np.max(dataset))\n",
        "                    labelset=np.array(labelset)\n",
        "                    if state == 'initial':\n",
        "                        with h5py.File('processed_signals.hdf5','w') as f:\n",
        "                            f.create_dataset('xtrain',data=dataset[:3600],maxshape=(None,129,11,12),chunks=True)\n",
        "                            f.create_dataset('xtest',data=dataset[3600:],maxshape=(None,129,11,12),chunks=True)\n",
        "                            f.create_dataset('ytrain',data=labelset[:3600],maxshape=(None,26),chunks=True)\n",
        "                            f.create_dataset('ytest',data=labelset[3600:],maxshape=(None,26),chunks=True)\n",
        "                        state='later'\n",
        "                    else:\n",
        "                        with h5py.File('processed_signals.hdf5','a') as f:\n",
        "                            f['xtrain'].resize((f['xtrain'].shape[0] + dataset[:3600].shape[0]), axis = 0)\n",
        "                            f['xtrain'][-dataset[:3600].shape[0]:] = dataset[:3600]\n",
        "                            f['xtest'].resize((f['xtest'].shape[0] + dataset[3600:].shape[0]), axis = 0)\n",
        "                            f['xtest'][-dataset[3600:].shape[0]:] = dataset[3600:]\n",
        "                            f['ytrain'].resize((f['ytrain'].shape[0] + labelset[:3600].shape[0]), axis = 0)\n",
        "                            f['ytrain'][-labelset[:3600].shape[0]:] = labelset[:3600]\n",
        "                            f['ytest'].resize((f['ytest'].shape[0] + labelset[3600:].shape[0]), axis = 0)\n",
        "                            f['ytest'][-labelset[3600:].shape[0]:] = labelset[3600:]\n",
        "                    dataset=[]\n",
        "                    labelset=[]\n",
        "                continue\n",
        "            break\n",
        "last=len(dataset)\n",
        "if len!=0:\n",
        "    dataset=np.array(dataset)\n",
        "    labelset=np.array(labelset)\n",
        "    m=np.floor(last*0.9).astype(int)\n",
        "    with h5py.File('processed_signals.hdf5','a') as f:\n",
        "        f['xtrain'].resize((f['xtrain'].shape[0] + dataset[:m].shape[0]), axis = 0)\n",
        "        f['xtrain'][-dataset[:m].shape[0]:] = dataset[:m]\n",
        "        f['xtest'].resize((f['xtest'].shape[0] + dataset[m:].shape[0]), axis = 0)\n",
        "        f['xtest'][-dataset[m:].shape[0]:] = dataset[m:]\n",
        "        f['ytrain'].resize((f['ytrain'].shape[0] + labelset[:m].shape[0]), axis = 0)\n",
        "        f['ytrain'][-labelset[:m].shape[0]:] = labelset[:m]\n",
        "        f['ytest'].resize((f['ytest'].shape[0] + labelset[m:].shape[0]), axis = 0)\n",
        "        f['ytest'][-labelset[m:].shape[0]:] = labelset[m:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "998 -35.77914023740884 8.490071587019992\n",
            "998 -0.9672336471549472 1.0450032539554541\n",
            "2058 -36.59620665074283 7.347591452746212\n",
            "2058 -1.0043730295792195 0.9930723387611914\n",
            "3582 -34.60145629387581 7.495922339051675\n",
            "3582 -0.9137025588125369 0.9998146517750761\n",
            "4506 -33.04505113318659 7.449551126266987\n",
            "4506 -0.8429568696902995 0.9977068693757721\n",
            "5991 -32.126123365245256 7.534657877834003\n",
            "5991 -0.8011874256929662 1.0015753580833637\n",
            "7376 -33.16085157537557 7.645929280800916\n",
            "7376 -0.8482205261534349 1.0066331491273142\n",
            "8599 -37.64427832891158 8.307257414294455\n",
            "8599 -1.0520126513141628 1.036693518831566\n",
            "9819 -32.633188351509304 7.441727898042781\n",
            "9819 -0.8242358341595138 0.9973512680928537\n",
            "11234 -32.70900761621394 7.347384373121482\n",
            "11234 -0.8276821643733608 0.9930629260509765\n",
            "12746 -33.04505113318659 7.470582175859484\n",
            "12746 -0.8429568696902995 0.9986628261754311\n",
            "14049 -34.300315674143185 7.489390716726555\n",
            "14049 -0.9000143488246902 0.9995177598512069\n",
            "15324 -32.53356377647941 7.536716488890498\n",
            "15324 -0.8197074443854278 1.0016689313132046\n",
            "16834 -33.4569508045199 7.495139898005373\n",
            "16834 -0.8616795820236317 0.9997790862729715\n",
            "18207 -36.30276212595427 7.5186467596695525\n",
            "18207 -0.9910346420888303 1.0008475799849796\n",
            "19404 -30.76752258281597 7.6954679088917555\n",
            "19404 -0.7394328446734533 1.0088849049496253\n",
            "20644 -32.633188351509304 8.002868685726206\n",
            "20644 -0.8242358341595138 1.0228576675330094\n",
            "22108 -33.85085427459005 7.54270069949259\n",
            "22108 -0.8795842852086385 1.001940940886027\n",
            "23484 -35.85629006166804 7.318099818737493\n",
            "23484 -0.9707404573485473 0.9917318099426133\n",
            "24986 -32.72818335657322 7.836468069760563\n",
            "24986 -0.8285537889351463 1.0152940031709348\n",
            "26428 -30.936993444508747 7.4988266915958555\n",
            "26428 -0.7471360656594885 0.9999466677998116\n",
            "27820 -33.67423862671305 7.425952673267118\n",
            "27820 -0.8715563012142294 0.9966342124212327\n",
            "29167 -34.51948564625293 7.446495507664572\n",
            "29167 -0.9099766202842242 0.9975679776211169\n",
            "30780 -33.60444684115393 7.498294974315939\n",
            "30780 -0.8683839473251787 0.9999224988325427\n",
            "32323 -37.64427832891158 7.560188894010311\n",
            "32323 -1.0520126513141628 1.0027358588186503\n",
            "33388 -33.40472084768486 7.438267499297978\n",
            "33388 -0.8593054930765844 0.9971939772408173\n",
            "34831 -33.10107512342708 7.453938813790278\n",
            "34831 -0.8455034147012309 0.9979063097177399\n",
            "36030 -33.341937689444975 7.404967234613528\n",
            "36030 -0.8564517131565897 0.9956803288460695\n",
            "36923 -31.224436611682364 7.51879640290654\n",
            "36923 -0.7602016641673802 1.0008543819502973\n",
            "37992 -32.799160959837046 8.699538685760322\n",
            "37992 -0.8317800436289566 1.0545244857163782\n",
            "39202 -37.71270426872036 7.5792319446816405\n",
            "39202 -1.055122921305471 1.0036014520309837\n",
            "40172 -28.161580899166122 7.2946559980313195\n",
            "40172 -0.6209809499620964 0.9906661817286964\n",
            "41554 -34.42445704990562 7.836468069760563\n",
            "41554 -0.9056571386320736 1.0152940031709348\n",
            "42891 -32.88630007395038 7.661857518550897\n",
            "42891 -0.83574091245229 1.0073571599341316\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mmo66uQoMA1Q"
      },
      "source": [
        "import numpy as np\n",
        "from helper_code import *\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.signal as scisig\n",
        "from numpy import inf\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Dropout, GlobalMaxPooling2D, MaxPooling2D, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def create_model():\n",
        "    # Build the model using the functional API\n",
        "    i = Input((129,11,12))\n",
        "    # x = Conv2D(32, (3, 3), strides=2, activation='relu')(i)\n",
        "    # x = Conv2D(64, (3, 3), strides=2, activation='relu')(x)\n",
        "    # x = Conv2D(128, (3, 3), strides=2, activation='relu')(x)\n",
        "\n",
        "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(i)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    # x = Dropout(0.2)(x)\n",
        "    x = Conv2D(64,(3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    # x = Dropout(0.2)(x)\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    # x = Dropout(0.2)(x)\n",
        "\n",
        "    x = GlobalMaxPooling2D()(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(26, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(i, x)\n",
        "\n",
        "    return model\n",
        "\n",
        "#with tpu_strategy.scope():\n",
        "#with tf.device('/job:localhost/replica:0/task:0/device:CPU:0'):\n",
        "model = create_model()\n",
        "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DaZA6ruRY1D"
      },
      "source": [
        "preprocessed_training_data_location=\"/content/processed_signals.hdf5\"\n",
        "class My_Generator(tf.keras.utils.Sequence):\n",
        "    def __init__(self,data_location,batch_size,trainortest):\n",
        "        self.data_location=data_location\n",
        "        self.batch_size=batch_size\n",
        "        self.trainortest=trainortest\n",
        "\n",
        "    def __len__(self):\n",
        "        with h5py.File('processed_signals.hdf5','r')as f:\n",
        "            len=f['xtrain'].shape[0]\n",
        "            num_batches=np.ceil(len/self.batch_size).astype(int)\n",
        "        return num_batches\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        with h5py.File('processed_signals.hdf5','r')as f:\n",
        "            if self.trainortest=='train':\n",
        "                x=f['xtrain'][idx*self.batch_size:(idx+1)*self.batch_size]\n",
        "                y=f['ytrain'][idx*self.batch_size:(idx+1)*self.batch_size]\n",
        "                return np.array(x),np.array(y)\n",
        "            else:\n",
        "                x=f['xtest'][idx*self.batch_size:(idx+1)*self.batch_size]\n",
        "                y=f['ytest'][idx*self.batch_size:(idx+1)*self.batch_size]\n",
        "                return np.array(x),np.array(y)\n",
        "\n",
        "my_training_batch_generator = My_Generator(preprocessed_training_data_location,1000,'train')\n",
        "my_testing_batch_generator = My_Generator(preprocessed_training_data_location,1000,'test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjiHPGPjo07Y",
        "outputId": "b8ffb97a-7693-488d-a7ff-dc430eb936df"
      },
      "source": [
        "import h5py\n",
        "f=model.fit(my_training_batch_generator,validation_data = my_testing_batch_generator,epochs=20)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "120/120 [==============================] - 679s 5s/step - loss: 0.1510 - accuracy: 0.2511 - val_loss: 0.3632 - val_accuracy: 0.0111\n",
            "Epoch 2/20\n",
            "120/120 [==============================] - 593s 5s/step - loss: 0.1257 - accuracy: 0.2962 - val_loss: 0.1405 - val_accuracy: 0.2417\n",
            "Epoch 3/20\n",
            "120/120 [==============================] - 575s 5s/step - loss: 0.1179 - accuracy: 0.3198 - val_loss: 0.1541 - val_accuracy: 0.0636\n",
            "Epoch 4/20\n",
            "120/120 [==============================] - 576s 5s/step - loss: 0.1117 - accuracy: 0.3378 - val_loss: 0.1538 - val_accuracy: 0.1447\n",
            "Epoch 5/20\n",
            "120/120 [==============================] - 586s 5s/step - loss: 0.1085 - accuracy: 0.3462 - val_loss: 0.1820 - val_accuracy: 0.0851\n",
            "Epoch 6/20\n",
            "120/120 [==============================] - 609s 5s/step - loss: 0.1055 - accuracy: 0.3481 - val_loss: 0.1499 - val_accuracy: 0.2628\n",
            "Epoch 7/20\n",
            "120/120 [==============================] - 595s 5s/step - loss: 0.1034 - accuracy: 0.3623 - val_loss: 0.1440 - val_accuracy: 0.2098\n",
            "Epoch 8/20\n",
            "120/120 [==============================] - 607s 5s/step - loss: 0.1009 - accuracy: 0.3681 - val_loss: 0.1395 - val_accuracy: 0.2491\n",
            "Epoch 9/20\n",
            "120/120 [==============================] - 597s 5s/step - loss: 0.0996 - accuracy: 0.3726 - val_loss: 0.1577 - val_accuracy: 0.2699\n",
            "Epoch 10/20\n",
            "120/120 [==============================] - 612s 5s/step - loss: 0.0976 - accuracy: 0.3847 - val_loss: 0.1952 - val_accuracy: 0.0917\n",
            "Epoch 11/20\n",
            "120/120 [==============================] - 594s 5s/step - loss: 0.0972 - accuracy: 0.3775 - val_loss: 0.1180 - val_accuracy: 0.2874\n",
            "Epoch 12/20\n",
            "120/120 [==============================] - 592s 5s/step - loss: 0.0949 - accuracy: 0.3949 - val_loss: 0.2072 - val_accuracy: 0.2666\n",
            "Epoch 13/20\n",
            "120/120 [==============================] - 606s 5s/step - loss: 0.0934 - accuracy: 0.3985 - val_loss: 0.1728 - val_accuracy: 0.2564\n",
            "Epoch 14/20\n",
            "120/120 [==============================] - 598s 5s/step - loss: 0.0918 - accuracy: 0.4083 - val_loss: 0.1052 - val_accuracy: 0.3055\n",
            "Epoch 15/20\n",
            "120/120 [==============================] - 620s 5s/step - loss: 0.0916 - accuracy: 0.4045 - val_loss: 0.1446 - val_accuracy: 0.2728\n",
            "Epoch 16/20\n",
            "120/120 [==============================] - 595s 5s/step - loss: 0.0908 - accuracy: 0.4148 - val_loss: 0.2739 - val_accuracy: 0.0180\n",
            "Epoch 17/20\n",
            "120/120 [==============================] - 610s 5s/step - loss: 0.0896 - accuracy: 0.4251 - val_loss: 0.1340 - val_accuracy: 0.2120\n",
            "Epoch 18/20\n",
            "120/120 [==============================] - 603s 5s/step - loss: 0.0878 - accuracy: 0.4208 - val_loss: 0.1308 - val_accuracy: 0.3242\n",
            "Epoch 19/20\n",
            "120/120 [==============================] - 610s 5s/step - loss: 0.0861 - accuracy: 0.4300 - val_loss: 0.1402 - val_accuracy: 0.2943\n",
            "Epoch 20/20\n",
            "120/120 [==============================] - 607s 5s/step - loss: 0.0845 - accuracy: 0.4425 - val_loss: 0.2212 - val_accuracy: 0.3461\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6fZRgt8PS11"
      },
      "source": [
        "final_dataset=np.zeros()\n",
        "header_files,recording_files=find_challenge_files(\"/content/training_data\")\n",
        "dataset=[]\n",
        "labelset=[]\n",
        "labels=[\"164889003\",\"164890007\",\"6374002\",\"426627000\",\"733534002\",\"713427006\",\"270492004\",\"713426002\",\"39732003\",\"445118002\",\"251146004\",\"698252002\",\"426783006\",\"284470004\",\"10370003\",\"365413008\",\"427172004\",\"164947007\",\"111975006\",\"164917005\",\"47665007\",\"427393009\",\"426177001\",\"427084000\",\"164934002\",\"59931005\"]\n",
        "for i in range(len(header_files)):\n",
        "    header=load_header(header_files[i])\n",
        "    recording_id=get_recording_id(header)\n",
        "    recording=load_recording(recording_files[i])\n",
        "    frequency=get_frequency(header)\n",
        "    num_samples=get_num_samples(header)\n",
        "    num_needed_samples=int(5*frequency)\n",
        "    num_signals_in_one_recording=int(num_samples//num_needed_samples)\n",
        "    current_labels=get_labels(header)\n",
        "    current_labels=[\"733534002\" if i == \"164909002\" else \"713427006\" if i == \"59118001\" else \"284470004\" if i == \"63593006\" else \"427172004\" if i == \"17338001\" else i for i in current_labels ]\n",
        "    label=np.zeros(26)\n",
        "    label_indices = [i for i in range(len(labels)) if labels[i] in current_labels]\n",
        "    label[label_indices]=1\n",
        "    if ((num_needed_samples<=int(num_samples))):\n",
        "        leads=choose_leads(recording,header,['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6'])\n",
        "        for j in range(num_signals_in_one_recording):\n",
        "            temp=np.zeros((129,11,12))\n",
        "            for k in range(len(leads)):\n",
        "                signal=leads[k][(j)*num_needed_samples:(j+1)*num_needed_samples]\n",
        "                signal=scisig.resample(signal,2500)\n",
        "                _,_,data=scisig.spectrogram(signal,fs=frequency)\n",
        "                if np.all(data) == False:\n",
        "                    break\n",
        "                data=(np.log10(data))\n",
        "                temp[:,:,k]=data\n",
        "            else:\n",
        "                dataset+=[temp]\n",
        "                labelset+=[label]\n",
        "                if len(dataset)%4000==0:\n",
        "                    print(i)\n",
        "                    dataset=(np.array(dataset)+14.5)/44\n",
        "                    labelset=np.array(labelset)\n",
        "                    f=model.fit(dataset,labelset,batch_size=64,validation_split=0.8,epochs=20)\n",
        "                    dataset=[]\n",
        "                    labelset=[]\n",
        "                continue\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}